{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the necessary packages installed, we import our necessary packages for accessing files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Install any of the following that you don't already have installed.\r\n",
    "import os.path\r\n",
    "import datetime\r\n",
    "import time\r\n",
    "import pandas as pd\r\n",
    "from os import path\r\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we import our `twarc` library, set our keys, and initiate our `Twarc` object. Fill out the blank strings with your actual keys from the Twitter developer website."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from twarc import Twarc\r\n",
    "\r\n",
    "consumer_key = \"\"\r\n",
    "consumer_secret = \"\"\r\n",
    "access_token = \"\"\r\n",
    "access_token_secret = \"\"\r\n",
    "\r\n",
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret, tweet_mode = \"extended\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we load in the IDs of our dehydrated tweets that we seek to rehydrate. We'll also look at the set of Tweets that have been already hydrated to see where we have to pick up. Check the output to see how much of the total set of IDs has been hydrated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dehydrated_ids = list(pd.read_csv(\"id_chunk.csv\")[\"id\"])\r\n",
    "already_hydrated_ids = []\r\n",
    "hydrated_df = pd.DataFrame(columns = [\"id_str\", \"rt\", \"full_text\", \"created_at\", \"user_id\", \"lang\", \"location\", \"country_code\", \"place_name\"])\r\n",
    "\r\n",
    "if path.exists(\"hydrated_df.csv\"):\r\n",
    "    hydrated_df = pd.read_csv(\"hydrated_df.csv\")\r\n",
    "    already_hydrated_ids = list(hydrated_df[\"id\"])\r\n",
    "    print(\"Hydrated \" + str(len(already_hydrated_ids)) + \" tweets so far\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we print the total percent of IDs that we've hydrated. This is not going to track one-to-one with the amount of Tweets that are in the resulting dataset since some Tweets may have been deleted or marked private since their collection."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ids_to_hydrate = list(set(dehydrated_ids) - set(already_hydrated_ids))\r\n",
    "print(str(round(100*len(already_hydrated_ids)/(len(already_hydrated_ids) + len(ids_to_hydrate)), 3)) + \"% of total IDs hydrated\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hydration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we'll hydrate our tweetset and collect our desired information. We write to our CSVs in real time since practically this script is going to be closed and rerun multiple times. The following chunk is a helper function that will save our interim extracted Tweets along with their data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save_new_tweets(filename: str, existing_data: pd.DataFrame, new_data: dict):\r\n",
    "    ''' Given a filename, existing dataset, and new data, this function will\r\n",
    "    append the new data to the existing dataset and save this new combined\r\n",
    "    dataset under the desired name. The function will then empty the columns\r\n",
    "    of the intermediate dataset.\r\n",
    "    '''\r\n",
    "    existing_data = existing_data.append(pd.DataFrame(new_data))\r\n",
    "    existing_data.to_csv(filename, index = False)\r\n",
    "    for key in new_data.keys():\r\n",
    "        new_data[key] = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And this helper function will assign the values from the Tweet object to a running dictionary containing our new values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def assign_attributes(tweet, attributes_to_assign, existing_dict):\r\n",
    "    ''' Given a Twarc tweet object, a set of attributes to assign,\r\n",
    "    and an existing dictionary, this function will assign the \r\n",
    "    desired attributes from the tweet to the dictionary.\r\n",
    "    '''\r\n",
    "    for attribute in attributes_to_assign:\r\n",
    "        original_attribute_name = attribute\r\n",
    "        object_to_access = tweet\r\n",
    "        if attribute == \"full_text\": # If we are trying to extract the full text of a Tweet then the way we do so depends on whether it is a Retweet\r\n",
    "            object_to_access = tweet[\"retweeted_status\"] if is_rt else tweet\r\n",
    "        if attribute in user_attributes: # If we are trying to extract an attribute tied to a user, we access the user object nested within the Tweet object\r\n",
    "            object_to_access = tweet[\"user\"]\r\n",
    "            attribute = \"id\" if attribute == \"user_id\" else attribute # We rename specific attributes with names that would have been confusing outside of the object\r\n",
    "        elif attribute in place_attributes: # Similarly, we may access the place object within the Tweet object\r\n",
    "            object_to_access = tweet[\"place\"]\r\n",
    "            attribute = \"full_name\" if attribute == \"place_name\" else attribute\r\n",
    "        # If the object we need to access is missing, most likely place, we assign a None value. Otherwise, we access the correct attribute from the given object\r\n",
    "        new_value = None if object_to_access == None else object_to_access[attribute] \r\n",
    "        existing_dict[original_attribute_name].append(new_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we iterate through our IDs to hydrate and do so."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First, we initialize the set of attributes we will be extracting from each Tweet\r\n",
    "newly_hydrated_data = {\r\n",
    "    \"id_str\": [], # Tweet attribute\r\n",
    "    \"rt\": [], # Tweet attribute\r\n",
    "    \"full_text\": [], # Tweet attribute\r\n",
    "    \"created_at\": [], # Tweet attribute\r\n",
    "    \"user_id\": [], # User attribute\r\n",
    "    \"lang\": [], # User attribute\r\n",
    "    \"location\": [], # User attribute\r\n",
    "    \"country_code\": [], # Place attribute\r\n",
    "    \"place_name\": [] # Place attribute\r\n",
    "}\r\n",
    "\r\n",
    "# Also, since they are accessed differently within a Tweet object, we define our user and place attributes.\r\n",
    "user_attributes = [\"user_id\", \"lang\", \"location\"]\r\n",
    "place_attributes = [\"country_code\", \"place_name\"]\r\n",
    "attributes_to_assign = set(newly_hydrated_data.keys()) - set([\"rt\"]) # Since RTs are not actually Twitter API Tweet object attributes, we differentiate between it and the other attributes.\r\n",
    "\r\n",
    "# We set a counter which will tell us when to wait for the Twitter API\r\n",
    "counter = 0\r\n",
    "\r\n",
    "# Now we iterate through our ids_to_hydrate using Twarc and extract the needed values\r\n",
    "for tweet in t.hydrate(ids_to_hydrate):\r\n",
    "    counter += 1\r\n",
    "    is_rt = \"retweeted_status\" in tweet.keys() # As it is not an attribute in the Twitter API Tweet object, we manually extract whether a given Tweet is a Retweet\r\n",
    "    newly_hydrated_data[\"rt\"].append(is_rt) # We assign the value we found\r\n",
    "    assign_attributes(tweet = tweet, attributes_to_assign = attributes_to_assign, existing_dict = newly_hydrated_data) # And assign the attributes we desire using the above function\r\n",
    "    if (counter % 900) == 0: # Every 900 Tweets, we stop and save our Tweets\r\n",
    "        save_new_tweets(filename = \"hydrated_df.csv\", existing_data = hydrated_df, new_data = newly_hydrated_data)\r\n",
    "        clear_output()\r\n",
    "        print(str(counter) + \" new tweets hydrated.\")\r\n",
    "        time.sleep(900) # And we wait 900 seconds to comply with the Twitter API's 900 Tweets / 15 minutes rate limit\r\n",
    "\r\n",
    "# If we get out of the loop, we have hydrated all ids_to_hydrate and so we save what remains and print that we have finished\r\n",
    "save_new_tweets(filename = \"hydrated_df.csv\", existing_data = hydrated_df, new_data = newly_hydrated_data)\r\n",
    "print(\"All tweets saved, you're done!\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "4138dce0fa74a6453ad2e7bd72e4222e8a2660a917e8bc73b32b3d62a6a94433"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}